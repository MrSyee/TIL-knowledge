# Diffusion Transformers (DiTs)

## List
- Scalable Diffusion Models with Transformers (2022.12) [[Paper](https://arxiv.org/abs/2212.09748)] [[Summary](image-generation/DiT/DiT.md)]
    - Diffusion UNet을 Transformers 구조로 대체한 DiT 제안.
- Scaling Vision Transformers to 22 Billion Parameters (2023.02) [[Paper](https://arxiv.org/abs/2302.05442)]
    - Parallel attention layers 제안하여 Transformers 모델을 효율적으로 scale up 하는 방법 제안.
- Improving Image Generation with Better Captions (2020.06) [[Paper](https://cdn.openai.com/papers/dall-e-3.pdf)]
    - 학습 데이터를 위한 향상된 captioning 방법 제안 (DALLE-3 논문)
- Scaling Rectified Flow Transformers for High-Resolution Image Synthesis (2024.07) [[Paper](https://openreview.net/forum?id=FPnUhsQJ5B)]
    - Rectified Flows 방식 제안. FLUX 모델의 핵심 알고리즘.

### Reference for FLUX
- https://medium.com/@drmarcosv/how-does-flux-work-the-new-image-generation-ai-that-rivals-midjourney-7f81f6f354da