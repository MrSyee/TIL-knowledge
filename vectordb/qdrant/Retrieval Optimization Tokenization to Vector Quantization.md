# Retrieval Optimization: Tokenization to Vector Quantization.
- Lecture: https://learn.deeplearning.ai/courses/retrieval-optimization-from-tokenization-to-vector-quantization/lesson/1/introduction

## Introduction

- Vector Databases use specialized data structures to approximate the search of nearest neighbors.
- HNSW
    - Hierarchical
    - Navigable
    - Small words
- Product quantization
- Scalar quantization
- Binary quantization

## Embedding models

## Role of the tokenizers

- Encoding
    - BPE - Byte-Pair Encoding
      <img width="981" alt="image" src="https://github.com/user-attachments/assets/8243c522-a5bf-4410-9628-3dac6290a92f">

    - WordPiece
      <img width="959" alt="image" src="https://github.com/user-attachments/assets/80eacc58-d54c-4236-b757-31cf85d2f59f">

    - Unigram
      <img width="984" alt="image" src="https://github.com/user-attachments/assets/9485a596-1d16-46e9-adbb-c9394892e3e2">

- SenetencePiece
    - Implementation of the same tokenization algorithm
    - BPE or Unigram


## Practical implications of the tokenization

### SentenceTransformer
- Unknown tokens
    - emoji -> regular letters
        - "I feel ğŸ˜Š" -> "I feel happy"
- Identifiers

```
sbert_tokenizer.encode("Broadcom BCM2712").tokens
>>>
['[CLS]', 'broad', '##com', 'bc', '##m', '##27', '##12', '[SEP]']
```

- Typos

```
sentences = [
    "Great accommodation",
    "Great acommodation",
]
for sentence in sentences:
    print(sbert_tokenizer.encode(sentence).tokens)

sentences = [
    "Great accommodation",
    "Great acommodation",
]
for sentence in sentences:
    print(sbert_tokenizer.encode(sentence).tokens)
sentences = [
    "Great accommodation",
    "Great acommodation",
]
for sentence in sentences:
    print(sbert_tokenizer.encode(sentence).tokens)

>>>
['[CLS]', 'great', 'accommodation', '[SEP]']
['[CLS]', 'great', 'ac', '##om', '##mo', '##dation', '[SEP]']
```

- Numerical values and date/time

```
sentences = [
    "16th February 2024",
    "2024-02-16",
    "17th February 2024",
    "18th February 2024",
    "19th February 2024",
    "20th February 2024",
    "15th February 2024",
]
for sentence in sentences:
    print(sbert_tokenizer.encode(sentence).tokens)

>>>
['[CLS]', '16th', 'february', '202', '##4', '[SEP]']
['[CLS]', '202', '##4', '-', '02', '-', '16', '[SEP]']
['[CLS]', '17th', 'february', '202', '##4', '[SEP]']
['[CLS]', '18th', 'february', '202', '##4', '[SEP]']
['[CLS]', '19th', 'february', '202', '##4', '[SEP]']
['[CLS]', '20th', 'february', '202', '##4', '[SEP]']
['[CLS]', '15th', 'february', '202', '##4', '[SEP]']
```

### tiktoken
- OpenAIì˜ ì„ë² ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬
- ìœ„ ì˜¤ë¥˜ ì¼€ì´ìŠ¤ì— ëŒ€í•´ì„œ ëŒ€ì‘ ê°€ëŠ¥í•˜ë‹¤.

### Vector search in practice (w/ Qdrant)
- Qdrantë¡œ ë²¡í„° ê²€ìƒ‰ ì‹¤ìŠµ

## Measuring Search Relevance
- RAG applicationì˜ ê²€ìƒ‰ í€„ë¦¬í‹°ë¥¼ ì˜¬ë¦¬ê¸° ìœ„í•œ ê°•ì˜ì—¬ì„œ ìŠ¤í‚µí•¨.

## Optimizing HNSW search
- HNSWëŠ” vector DBì˜ ê¸°ì´ˆê°€ ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.
- To approximate the nearest-neighbor search.

### HNSW structure
- Hierarchical Navigable Samll Worlds is the most commonly algorithms to approximate nearest neighbor search.

- `m` parameters
    - Define how many edges should each node have.
    - ê°’ì´ ì¦ê°€í• ìˆ˜ë¡
        - search precision ì¦ê°€
        - latencyì— ì˜í–¥ì„ ì¤€ë‹¤. (ì„±ëŠ¥, ì†ë„ tradeoff)
- `ef` closest points
    - queryë¡œë¶€í„° ê°€ê¹Œìš´ í¬ì¸íŠ¸ë¥¼ `ef`ê°œ ì°¾ëŠ”ë‹¤.
    - ë‹¤ìŒ layerë¡œ ê°€ì„œ ë™ì¼í•œ ì‘ì—…ì„ ë°˜ë³µ

### ì‹¤ìŠµ

- ANN search

```
client.search(
    "wands-products",
    query_vector=models.NamedVector(
        name="product_name",
        vector=model.encode(queries_df.loc[0, "query"])
    ),
    limit=3,
    with_vectors=False,
    with_payload=False,
)
```

- kNN search
```
client.search(
    "wands-products",
    query_vector=models.NamedVector(
        name="product_name",
        vector=model.encode(queries_df.loc[0, "query"])
    ),
    limit=3,
    with_vectors=False,
    with_payload=False,
    search_params=models.SearchParams(
        exact=True,  # Turns on the exact search mode
    ),
)
```

- íŒŒë¼ë¯¸í„° m ê³¼ efë¥¼ ë†’íˆë©´ ì •í™•ë„ê°€ ì˜¬ë¼ê°„ë‹¤. ëŒ€ì‹  ì†ë„ëŠ” ëŠë ¤ì§.

## Vector quantization
- optimization
    - reducing memory

### Product Quantization(PQ)
- ê³ ì°¨ì›ì˜ ë²¡í„°ë¥¼ ì„œë¸Œë²¡í„°ë¡œ ë‚˜ëˆˆë‹¤.
- indexing ì‹œê°„ì€ ëŠ˜ì–´ë‚œë‹¤.

<img width="884" alt="image" src="https://github.com/user-attachments/assets/30d8fc75-4571-4158-b628-3f6a3cfbfe18">

- x16(4D clusters) ë¶€í„° search timeì´ ì¤„ì–´ë“ ë‹¤.


### Rescoring
- qunatized vertorë“¤ì„ íƒìƒ‰í•  ë•Œ, ë§ì€ documents ë“¤ì´ ê°™ì€ representationì„ ê°–ëŠ”ë‹¤.
- VectorDBëŠ” original vertorsë¥¼ ê·¸ëŒ€ë¡œ ë‘ê³  ê·¸ ë²¡í„°ì˜ ê²°ê³¼ë“¤ì„ rescore í•œë‹¤.


### Scalar Quantization(SQ)
- ê° ë²¡í„° ê°’ì˜ íƒ€ì…ì„ ë³€ê²½í•œë‹¤. e.g.  float -> int
- indexing and search timeì„ ì¤„ì¸ë‹¤. ëŒ€ì‹  precisionë„ ì¤„ì–´ë“ ë‹¤. ê·¸ëŸ°ë° precisionì´ ì•„ì£¼ ì•½ê°„ë°–ì— ì•ˆ ì¤„ì–´ë“ ë‹¤.
<img width="978" alt="image" src="https://github.com/user-attachments/assets/9762a659-a11c-4e02-9145-61181f495062">

### Binary Qunatization(BQ)
- ë©”ëª¨ë¦¬ë¥¼ ì•„ë¼ê¸° ìœ„í•´ float ê°’ì„ boolean ê°’ìœ¼ë¡œ ë³€ê²½í•œë‹¤. float -> single bit
- precisionì´ í¬ê²Œ ì¤„ì–´ë“¤ì§€ë§Œ, rescoring ì´í›„ì—ëŠ” ê½¤ ë†’ì€ precisionì„ ë³´ì—¬ì¤€ë‹¤.
<img width="1010" alt="image" src="https://github.com/user-attachments/assets/c80f612b-8a5b-44e1-ba54-71d2bee68c7c">

### Summary
- precision: SQ == SQ + rescoring > BQ + rescoring > PQ + rescoring
- speed: BQ > SQ > PQ

